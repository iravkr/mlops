# ğŸµ Audio Sentiment Analysis MLOps Project

A complete end-to-end MLOps pipeline for audio sentiment analysis. This project classifies audio speech recordings as positive, negative, or neutral sentiment - perfect for customer service analysis, voice feedback systems, and emotional AI applications.

## ğŸš€ Quick Start

```bash
# 1. Clone and setup environment
git clone <your-repo>
cd mlops_zoomcamp
chmod +x setup.sh
./setup.sh
source venv/bin/activate

# 2. Download dataset (place in data/TRAIN/)
# Kaggle: https://www.kaggle.com/datasets/imsparsh/audio-speech-sentiment

# 3. Run complete pipeline
python src/pipeline.py

# 4. Test API
python src/deployment/api.py &
curl -X POST -F "file=@data/TRAIN/1.wav" http://localhost:8000/predict
```

## ğŸ“‹ Complete MLOps Pipeline Setup

### 1. Environment Setup
```bash
# Create virtual environment and install dependencies with uv (ultra-fast)
./setup.sh
source venv/bin/activate
```

### 2. Data Processing & Model Training
```bash
# Download Kaggle dataset to data/TRAIN/ directory
# Files: 1.wav, 2.wav, ... and TRAIN.csv with labels

# Train model with MLflow tracking (achieves 96% accuracy)
python src/training/train.py

# Start MLflow UI for experiment tracking
mlflow ui --host 0.0.0.0 --port 5001 &
# View experiments at: http://localhost:5001
```

### 3. API Deployment & Testing
```bash
# Start FastAPI server
python src/deployment/api.py &
# Or use uvicorn: uvicorn src.deployment.api:app --host 0.0.0.0 --port 8000

# Test health endpoint
curl http://localhost:8000/health

# Test prediction with audio file
curl -X POST -F "file=@data/TRAIN/5.wav" http://localhost:8000/predict
# Interactive API docs: http://localhost:8000/docs
```

### 4. Workflow Orchestration
```bash
# Run complete Prefect pipeline (training + monitoring + deployment)
python src/pipeline.py

# Pipeline includes:
# - Data validation and preprocessing
# - Model training with hyperparameter tuning
# - Model evaluation and registration
# - Data drift monitoring
# - API health checks
```

### 5. Testing & Validation
```bash
# Run comprehensive test suite (5 tests covering all components)
python -m pytest tests/ -v

# Tests include:
# - Data processing functionality
# - Model training pipeline
# - API endpoints and error handling
# - Monitoring system validation
# - Integration tests
```

### 6. Containerization
```bash
# Build Docker image with optimized uv installation
docker build -t audio-sentiment-api .

# Run containerized API
docker run -p 8000:8000 audio-sentiment-api

# Or use docker-compose for full stack (API + MLflow)
docker-compose up
# Services: API (8000), MLflow (5001)
```

### 7. Cloud Deployment (AWS)
```bash
# Configure AWS credentials
aws configure

# Deploy infrastructure with Terraform
cd infrastructure
./deploy.sh
# Creates: ECR repository, S3 buckets, IAM roles

# Build and push Docker image to ECR
cd ..
./scripts/deploy_to_aws.sh
# Pushes image to: <account>.dkr.ecr.us-east-1.amazonaws.com/audio-sentiment-repo

# Alternative deployment options:
# - EC2 instances with Docker
# - AWS Lambda Container Images
# - Other cloud providers
```

### 8. Infrastructure Cleanup
```bash
# Destroy AWS resources to avoid charges
cd infrastructure
terraform destroy -auto-approve

# Remove local containers
docker system prune -a
```

## ğŸ“ Project Structure

```
mlops_zoomcamp/
â”œâ”€â”€ ğŸ“Š src/
â”‚   â”œâ”€â”€ data_processing/       # Audio feature extraction and validation
â”‚   â”‚   â””â”€â”€ data_processing.py # MFCC, spectral features extraction
â”‚   â”œâ”€â”€ training/             # Model training with MLflow
â”‚   â”‚   â””â”€â”€ train.py          # Random Forest training pipeline
â”‚   â”œâ”€â”€ deployment/           # FastAPI REST API service
â”‚   â”‚   â””â”€â”€ api.py            # Production-ready API with health checks
â”‚   â”œâ”€â”€ monitoring/           # Data drift detection
â”‚   â”‚   â””â”€â”€ monitor.py        # Evidently-based monitoring with fallbacks
â”‚   â””â”€â”€ pipeline.py           # Prefect orchestration workflow
â”œâ”€â”€ ğŸ§ª tests/                 # Comprehensive test suite (5 tests)
â”‚   â”œâ”€â”€ test_data_processing.py
â”‚   â”œâ”€â”€ test_training.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_monitoring.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ ğŸ—ï¸ infrastructure/        # AWS Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf               # Terraform AWS resources
â”‚   â”œâ”€â”€ variables.tf          # Configuration variables
â”‚   â”œâ”€â”€ outputs.tf            # Infrastructure outputs
â”‚   â”œâ”€â”€ deploy.sh             # Automated deployment script
â”‚   â””â”€â”€ ecs-task-definition.json
â”œâ”€â”€ ğŸ³ docker/               # Containerization
â”‚   â”œâ”€â”€ Dockerfile            # Multi-stage production build
â”‚   â”œâ”€â”€ docker-compose.yml    # Local development stack
â”‚   â””â”€â”€ .dockerignore         # Optimized build context
â”œâ”€â”€ ğŸ“œ scripts/              # Automation scripts
â”‚   â”œâ”€â”€ deploy_to_aws.sh      # ECR deployment automation
â”‚   â””â”€â”€ download_data.py      # Dataset download helper
â”œâ”€â”€ ğŸ“‚ data/                 # Dataset directory
â”‚   â””â”€â”€ TRAIN/               # Audio files (1.wav, 2.wav...) + TRAIN.csv
â”œâ”€â”€ ğŸ¤– models/               # Saved model artifacts
â”œâ”€â”€ ğŸ“‹ requirements.txt       # Python dependencies
â”œâ”€â”€ âš™ï¸ setup.sh               # Environment setup script
â”œâ”€â”€ ğŸ“– README.md             # This comprehensive guide
â”œâ”€â”€ ğŸ“Š DEPLOYMENT_SUMMARY.md # Cloud deployment details
â””â”€â”€ ğŸ”§ pyproject.toml        # Project configuration
```

## ğŸ“Š Dataset Information

**Source**: Kaggle Audio Speech Sentiment Dataset
- **Format**: WAV audio files + CSV labels
- **Location**: `data/TRAIN/*.wav` (numbered files: 1.wav, 2.wav, etc.)
- **Labels**: `data/TRAIN.csv` with columns `Filename,Class`
- **Classes**: Positive, Negative, Neutral
- **Download**: https://www.kaggle.com/datasets/imsparsh/audio-speech-sentiment
- **Size**: ~1000 audio samples for training and validation

## ğŸ¯ Model Performance & Architecture

- **Algorithm**: Random Forest Classifier with audio feature engineering
- **Features**: MFCCs, Spectral Centroid, Zero-Crossing Rate, RMS Energy
- **Accuracy**: 96% on validation set
- **Training Time**: ~2-3 minutes on standard hardware
- **Model Size**: ~50MB (saved as pickle)
- **Inference Speed**: <2 seconds per audio file

## ğŸ› ï¸ Technology Stack & Versions

- **Python**: 3.9.6+ (optimized for ML workloads)
- **Package Manager**: uv (ultra-fast Python package installation)
- **ML Framework**: scikit-learn + librosa for audio processing
- **Experiment Tracking**: MLflow 2.5.0 (model versioning & metrics)
- **API Framework**: FastAPI (async, auto-docs, type hints)
- **Data Monitoring**: Evidently 0.7.11 (drift detection with fallbacks)
- **Workflow Orchestration**: Prefect 3.4.10 (modern data workflows)
- **Testing**: Pytest (comprehensive test coverage)
- **Containerization**: Docker (multi-stage builds, security best practices)
- **Infrastructure**: AWS + Terraform (S3, ECR, IAM, optional ECS)
- **CI/CD**: GitHub Actions ready (workflow templates included)

## ğŸŒ Service URLs & Endpoints

When running the complete stack locally:

| Service | URL | Description |
|---------|-----|-------------|
| **API Server** | http://localhost:8000 | Main prediction endpoint |
| **API Documentation** | http://localhost:8000/docs | Interactive Swagger docs |
| **Health Check** | http://localhost:8000/health | Service health status |
| **MLflow UI** | http://localhost:5001 | Experiment tracking dashboard |
| **Metrics** | http://localhost:8000/metrics | Prometheus-style metrics |

### API Endpoints

```bash
# Health check
GET /health

# Predict sentiment from audio file
POST /predict
Content-Type: multipart/form-data
Body: file=@audio_file.wav

# Get model information
GET /model/info

# Response format:
{
  "sentiment": "positive|negative|neutral",
  "confidence": 0.95,
  "model_version": "1.0.0",
  "processing_time_ms": 1250
}
```

## âœ… MLOps Best Practices Implemented

| Category | Implementation | Status |
|----------|---------------|--------|
| **Experiment Tracking** | MLflow for model versioning, metrics, and artifacts | âœ… |
| **Workflow Orchestration** | Prefect for automated, schedulable pipelines | âœ… |
| **Data Monitoring** | Evidently for drift detection with statistical fallbacks | âœ… |
| **API Development** | FastAPI with async support, docs, and error handling | âœ… |
| **Testing Strategy** | Unit, integration, and API tests with pytest | âœ… |
| **Containerization** | Docker with multi-stage builds and security scanning | âœ… |
| **Infrastructure as Code** | Terraform for reproducible AWS deployments | âœ… |
| **CI/CD Pipeline** | GitHub Actions workflows for automated deployment | âœ… |
| **Environment Management** | Virtual environments with pinned dependencies | âœ… |
| **Documentation** | Comprehensive README, API docs, and inline comments | âœ… |
| **Model Governance** | Model versioning, performance tracking, rollback capability | âœ… |
| **Security** | Container scanning, IAM roles, encrypted storage | âœ… |

## ğŸš¨ Troubleshooting

### Common Issues

1. **Audio file format errors**
   ```bash
   # Ensure WAV format, convert if needed:
   ffmpeg -i input.mp3 output.wav
   ```

2. **MLflow tracking issues**
   ```bash
   # Clear MLflow cache
   rm -rf mlruns/
   rm -rf .mlflow/
   ```

3. **Docker build failures**
   ```bash
   # Clean Docker cache
   docker system prune -a
   docker build --no-cache -t audio-sentiment-api .
   ```

4. **AWS permission errors**
   ```bash
   # Verify AWS credentials
   aws sts get-caller-identity
   ```

### Performance Optimization

- **Model Loading**: Models are cached in memory for faster inference
- **Audio Processing**: Optimized librosa settings for speed vs. quality
- **Container Size**: Multi-stage builds reduce image size by 60%
- **API Performance**: Async FastAPI with connection pooling

## ğŸ“ˆ Monitoring & Observability

- **Data Drift**: Automatic detection with Evidently statistical tests
- **Model Performance**: Real-time accuracy and confidence tracking
- **API Metrics**: Request latency, error rates, throughput
- **Infrastructure**: AWS CloudWatch integration (when deployed)
- **Alerts**: Configurable thresholds for drift and performance degradation

## ğŸ”„ Development Workflow

1. **Feature Development**: Create feature branch
2. **Testing**: Run `pytest tests/` locally
3. **Docker Testing**: Build and test container locally
4. **Infrastructure**: Test Terraform changes in dev environment
5. **Deployment**: Merge to main triggers automated deployment
6. **Monitoring**: Monitor metrics and model performance
7. **Iteration**: Use MLflow experiments for A/B testing

## ğŸ“ Support & Contributing

- **Issues**: Use GitHub Issues for bug reports
- **Features**: Submit PRs with comprehensive tests
- **Documentation**: Update README for significant changes
- **Performance**: Profile with `cProfile` for optimization PRs

---

